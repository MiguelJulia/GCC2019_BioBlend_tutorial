{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1 - Answers for Galaxy API\n",
    "===================================\n",
    "\n",
    "**Goal**: Upload a file to a new history, import a workflow and run it on the uploaded dataset.\n",
    "\n",
    "1) Define the connection parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "\n",
    "import requests\n",
    "from six.moves.urllib.parse import urljoin\n",
    "\n",
    "server = 'https://usegalaxy.org/'\n",
    "api_key = '1a6625037e2a986f36dc17e0324743fa'\n",
    "base_url = urljoin(server, 'api')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Create a new Galaxy history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'annotation': None,\n",
       " u'contents_url': u'/api/histories/c5d5276103ac10f1/contents',\n",
       " u'create_time': u'2019-07-02T14:48:13.162661',\n",
       " u'deleted': False,\n",
       " u'empty': True,\n",
       " u'genome_build': None,\n",
       " u'id': u'c5d5276103ac10f1',\n",
       " u'importable': False,\n",
       " u'model_class': u'History',\n",
       " u'name': u'New history',\n",
       " u'published': False,\n",
       " u'purged': False,\n",
       " u'size': 0,\n",
       " u'slug': None,\n",
       " u'state': u'new',\n",
       " u'state_details': {u'discarded': 0,\n",
       "  u'empty': 0,\n",
       "  u'error': 0,\n",
       "  u'failed_metadata': 0,\n",
       "  u'new': 0,\n",
       "  u'ok': 0,\n",
       "  u'paused': 0,\n",
       "  u'queued': 0,\n",
       "  u'running': 0,\n",
       "  u'setting_metadata': 0,\n",
       "  u'upload': 0},\n",
       " u'state_ids': {u'discarded': [],\n",
       "  u'empty': [],\n",
       "  u'error': [],\n",
       "  u'failed_metadata': [],\n",
       "  u'new': [],\n",
       "  u'ok': [],\n",
       "  u'paused': [],\n",
       "  u'queued': [],\n",
       "  u'running': [],\n",
       "  u'setting_metadata': [],\n",
       "  u'upload': []},\n",
       " u'tags': [],\n",
       " u'update_time': u'2019-07-02T14:48:13.162673',\n",
       " u'url': u'/api/histories/c5d5276103ac10f1',\n",
       " u'user_id': u'2841a31c8997a2d3',\n",
       " u'username_and_slug': None}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'key': api_key}\n",
    "data = {'name': 'New history'}\n",
    "r = requests.post(base_url + '/histories', json.dumps(data), params=params, headers={'Content-Type': 'application/json'})\n",
    "new_hist = r.json()\n",
    "new_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) **Upload** the local file \"~/bioblend-tutorial/test-data/1.txt\" to a new dataset in the created history. You need to run the special 'upload1' tool by making a `POST` request to `/api/tools`. You don't need to pass any inputs to it apart from attaching the file as 'files_0|file_data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'implicit_collections': [],\n",
       " u'jobs': [{u'create_time': u'2019-07-02T14:48:18.943867',\n",
       "   u'exit_code': None,\n",
       "   u'history_id': u'c5d5276103ac10f1',\n",
       "   u'id': u'bbd44e69cb8906b58fd9ba7eb07afdd8',\n",
       "   u'model_class': u'Job',\n",
       "   u'state': u'new',\n",
       "   u'tool_id': u'upload1',\n",
       "   u'update_time': u'2019-07-02T14:48:19.153335'}],\n",
       " u'output_collections': [],\n",
       " u'outputs': [{u'create_time': u'2019-07-02T14:48:18.739845',\n",
       "   u'data_type': u'galaxy.datatypes.data.Data',\n",
       "   u'deleted': False,\n",
       "   u'file_ext': u'auto',\n",
       "   u'file_size': 0,\n",
       "   u'genome_build': u'?',\n",
       "   u'hda_ldda': u'hda',\n",
       "   u'hid': 1,\n",
       "   u'history_content_type': u'dataset',\n",
       "   u'history_id': u'c5d5276103ac10f1',\n",
       "   u'id': u'bbd44e69cb8906b569ca5e103d37687f',\n",
       "   u'metadata_dbkey': u'?',\n",
       "   u'misc_blurb': None,\n",
       "   u'misc_info': None,\n",
       "   u'model_class': u'HistoryDatasetAssociation',\n",
       "   u'name': u'1.txt',\n",
       "   u'output_name': u'output0',\n",
       "   u'peek': None,\n",
       "   u'purged': False,\n",
       "   u'state': u'queued',\n",
       "   u'tags': [],\n",
       "   u'update_time': u'2019-07-02T14:48:18.896395',\n",
       "   u'uuid': u'f2082787-c005-475c-bb9d-ed02c649709b',\n",
       "   u'visible': True}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "params = {'key': api_key}\n",
    "data = {\n",
    "    'history_id': new_hist['id'],\n",
    "    'tool_id': 'upload1'}\n",
    "with open(os.path.join(os.environ['HOME'], \"bioblend-tutorial/test-data/1.txt\"), 'rb') as f:\n",
    "    files = {'files_0|file_data': f}\n",
    "    r = requests.post(base_url + '/tools', data, params=params, files=files)\n",
    "ret = r.json()\n",
    "ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Find the new uploaded dataset, either from the dict returned by the POST request or from the history contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'create_time': u'2019-07-02T14:48:18.739845',\n",
       " u'data_type': u'galaxy.datatypes.data.Data',\n",
       " u'deleted': False,\n",
       " u'file_ext': u'auto',\n",
       " u'file_size': 0,\n",
       " u'genome_build': u'?',\n",
       " u'hda_ldda': u'hda',\n",
       " u'hid': 1,\n",
       " u'history_content_type': u'dataset',\n",
       " u'history_id': u'c5d5276103ac10f1',\n",
       " u'id': u'bbd44e69cb8906b569ca5e103d37687f',\n",
       " u'metadata_dbkey': u'?',\n",
       " u'misc_blurb': None,\n",
       " u'misc_info': None,\n",
       " u'model_class': u'HistoryDatasetAssociation',\n",
       " u'name': u'1.txt',\n",
       " u'output_name': u'output0',\n",
       " u'peek': None,\n",
       " u'purged': False,\n",
       " u'state': u'queued',\n",
       " u'tags': [],\n",
       " u'update_time': u'2019-07-02T14:48:18.896395',\n",
       " u'uuid': u'f2082787-c005-475c-bb9d-ed02c649709b',\n",
       " u'visible': True}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hda = ret['outputs'][0]\n",
    "hda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) **Import a workflow** from the local file \"~/bioblend-tutorial/test-data/convert_to_tab.ga\" by making a `POST` request to `/api/workflows`. The only needed data is 'workflow', which must be a deserialized JSON representation of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'deleted': False,\n",
       " u'id': u'e29a08a43c4d93ab',\n",
       " u'latest_workflow_uuid': u'814e6545-db9e-4dc0-b9c4-1c0f9ed68910',\n",
       " u'model_class': u'StoredWorkflow',\n",
       " u'name': u'Convert to tab',\n",
       " u'number_of_steps': 2,\n",
       " u'owner': u'mjuliam',\n",
       " u'published': False,\n",
       " u'tags': [],\n",
       " u'url': u'/api/workflows/e29a08a43c4d93ab'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'key': api_key}\n",
    "with open(os.path.join(os.environ['HOME'], 'bioblend-tutorial/test-data/convert_to_tab.ga'), 'r') as f:\n",
    "    workflow_json = json.load(f)\n",
    "data = {'workflow': workflow_json}\n",
    "r = requests.post(base_url + '/workflows', json.dumps(data), params=params, headers={'Content-Type': 'application/json'})\n",
    "wf = r.json()\n",
    "wf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) View the details of the imported workflow by making a GET request to `/api/workflows`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'annotation': None,\n",
       " u'deleted': False,\n",
       " u'id': u'e29a08a43c4d93ab',\n",
       " u'inputs': {u'0': {u'label': u'Input Dataset',\n",
       "   u'uuid': u'671bca4e-0b76-4a6f-a0a2-70219df56576',\n",
       "   u'value': u''}},\n",
       " u'latest_workflow_uuid': u'814e6545-db9e-4dc0-b9c4-1c0f9ed68910',\n",
       " u'model_class': u'StoredWorkflow',\n",
       " u'name': u'Convert to tab',\n",
       " u'owner': u'mjuliam',\n",
       " u'published': False,\n",
       " u'steps': {u'0': {u'annotation': None,\n",
       "   u'id': 0,\n",
       "   u'input_steps': {},\n",
       "   u'tool_id': None,\n",
       "   u'tool_inputs': {u'name': u'Input Dataset'},\n",
       "   u'tool_version': None,\n",
       "   u'type': u'data_input'},\n",
       "  u'1': {u'annotation': None,\n",
       "   u'id': 1,\n",
       "   u'input_steps': {u'input': {u'source_step': 0, u'step_output': u'output'}},\n",
       "   u'tool_id': u'Convert characters1',\n",
       "   u'tool_inputs': {u'__page__': 0,\n",
       "    u'__rerun_remap_job_id__': None,\n",
       "    u'condense': u'true',\n",
       "    u'convert_from': u's',\n",
       "    u'input': None,\n",
       "    u'strip': u'true'},\n",
       "   u'tool_version': u'1.0.0',\n",
       "   u'type': u'tool'}},\n",
       " u'tags': [],\n",
       " u'url': u'/api/workflows/e29a08a43c4d93ab',\n",
       " u'version': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'key': api_key}\n",
    "r = requests.get(base_url + '/workflows/' + wf['id'], params)\n",
    "wf = r.json()\n",
    "wf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) **Run** the imported workflow on the uploaded dataset **inside the same history** by making a `POST` request to `/api/workflows`. The only needed data are 'workflow_id', 'history' and 'ds_map'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'0']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{u'history': u'c5d5276103ac10f1',\n",
       " u'history_id': u'c5d5276103ac10f1',\n",
       " u'id': u'36c00c836edfbc6c',\n",
       " u'inputs': {u'0': {u'id': u'bbd44e69cb8906b569ca5e103d37687f',\n",
       "   u'src': u'hda',\n",
       "   u'uuid': u'f2082787-c005-475c-bb9d-ed02c649709b'}},\n",
       " u'model_class': u'WorkflowInvocation',\n",
       " u'output_collections': {},\n",
       " u'outputs': [u'bbd44e69cb8906b542d15e93bc4cf528'],\n",
       " u'state': u'scheduled',\n",
       " u'steps': [{u'action': None,\n",
       "   u'id': u'b04a009f5ce0a4c2',\n",
       "   u'job_id': u'bbd44e69cb8906b5a1e02264b7256567',\n",
       "   u'model_class': u'WorkflowInvocationStep',\n",
       "   u'order_index': 1,\n",
       "   u'state': u'scheduled',\n",
       "   u'update_time': u'2019-07-02T14:48:40.133919',\n",
       "   u'workflow_step_id': u'70f9f57d9db9dfbb',\n",
       "   u'workflow_step_label': None,\n",
       "   u'workflow_step_uuid': u'dc5195e6-bf5e-4d1e-8868-21efae3ddc9c'},\n",
       "  {u'action': None,\n",
       "   u'id': u'30020b75052f6827',\n",
       "   u'job_id': None,\n",
       "   u'model_class': u'WorkflowInvocationStep',\n",
       "   u'order_index': 0,\n",
       "   u'state': u'scheduled',\n",
       "   u'update_time': u'2019-07-02T14:48:39.835215',\n",
       "   u'workflow_step_id': u'4800fcecd319eda1',\n",
       "   u'workflow_step_label': None,\n",
       "   u'workflow_step_uuid': u'671bca4e-0b76-4a6f-a0a2-70219df56576'}],\n",
       " u'update_time': u'2019-07-02T14:48:40.114389',\n",
       " u'uuid': u'7abe9d00-9cd8-11e9-827d-005056ba2773',\n",
       " u'workflow_id': u'b71fb0e86bc1c95c'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_step_ids = wf['inputs'].keys()\n",
    "print(input_step_ids)\n",
    "params = {'key': api_key}\n",
    "dataset_map = {input_step_ids[0]: {'id': hda['id'], 'src': 'hda'}}\n",
    "data = {\n",
    "    'workflow_id': wf['id'],\n",
    "    'history': 'hist_id=' + new_hist['id'],\n",
    "    'ds_map': dataset_map}\n",
    "r = requests.post(base_url + '/workflows', json.dumps(data), params=params, headers={'Content-Type': 'application/json'})\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) View the results on the Galaxy server with your web browser."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
